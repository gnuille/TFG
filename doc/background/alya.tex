\section{Alya and the combustion use case}
Alya is an HPC application for computational mechanics. It has diverse modules that solve a specific problem, which can be coupled to solve a general problem. For example, we can couple together the module to simulate how the temperature evolves in a system and the module that computes the particle motion in the same run. 

The code is written in Fortran90 and can use MPI, OpenMP/OmpSs and GPU. The module we are asked to optimize is pure MPI and despite the code is written in Fortran, it uses Cantera library for a critical computation inside the module. Cantera\cite{cantera} is an open-source suite of tools for problems involving chemical kinetics, thermodynamics, and transport processes written in C++.

\subsection{Compilation}

Alya is build using GNU/Make\cite{gnumake}. The Makefile necessary to compile the application is generated by a configure script written in Perl that parses the options and flags in the \textit{config.in} file. Table \ref{flagsintel} shows a summary of options toggled in the configuration. Notice that: we set the compiler to \textit{mpif90}, that is, the MPI compiler that wraps the Intel compiler,  we link to the Intel Math Kernel libraries\cite{intelmkl}, we explicitly tell the compiler that we want to use the Intel AVX-512 vectorial unit, to perform optimizations based on the architecture and to try to perform optimizations among all the files of the code.  In summary, we tried to achieve maximum performance from just tweaking compilation parameters.

\begin{table}[htbp]
\centering
%\refstepcounter{table}
\begin{tabular}{l|l|l} 
\toprule
\textbf{Flag}     & \textbf{Description}         & \textbf{Value}                                                                       \\ 
\hline
F77      & Fortran 77 compiler & mpif90                                                                      \\ 
\hline
F90      & Fortran 90 compiler & mpif90                                                                      \\ 
\hline
EXTRALIB & Libraries linked    & -lmkl\_core -lmkl\_sequential                                               \\ 
\hline 
FOPT     & Optimization level  & -O3                                                                         \\ 
\hline
OPTFLAGS & Optimization flags  & -xCORE-AVX512 -mtune=skylake -ipo                                           \\
\bottomrule
\end{tabular}
\caption{Configuration flags for Intel MPI Alya compilation.}
\label{flagsintel}
\end{table}

Once we have the \textit{config.in} file ready, we need to proceed to the actual compilation. Listing \ref{buildintel} shows the simplified build script. First, we load the Intel modules which enable the compilation using the Intel toolchain required. Then, we run the configure script, that will parse the \textit{config.in the} file we previously set and generate the Makefile. Finally using the generated Makefile we:

\begin{itemize}
  \item Compile the \textit{Metis4} partitioner.
  \item Compile the Cantera wrapper for Alya.
  \item Compile Alya telling the Makefile we enabled Cantera.
\end{itemize}

\begin{lstlisting}[language=sh, caption={Building Alya}, label={buildintel}]
module load intel mkl impi
./configure # -x nastin chemic parall temper
make metis4
make cantera4alya 
make CANTERA=1
\end{lstlisting}

With just that we got our Alya binary!

\subsection{First runs}

For running an HPC application, we need the binary and the input set. We know how to get the binary, and we are given the input set, meaning, we are ready for running the application.

An Alya input set is a folder with different files. Each file describes the mechanisms and parameters each module uses and defines the domain. All the input set files begin with the same name, and this is the case name. Listing \ref{runlinealya} shows the command to launch an Alya run assuming the binary is copied to the input set folder, and we are located inside that folder.

\begin{lstlisting}[language=sh, caption={Running Alya with 48 processes.}, label={runlinealya}]
mpirun -np 48 ./Alya.x CASE_NAME
\end{lstlisting}


Since we are running in a production HPC environment, we have to consider the resource manager to run isolated and with the requested amount of resources. Listing \ref{jobalya} shows how to run Alya using a SLURM script, the MareNostrum4 resource manager.

\begin{lstlisting}[language=sh, caption={Running Alya with 48 processes.}, label={jobalya}]
#!/bin/bash
#SBATCH --job-name=alya-test
#SBATCH --output=alya.out
#SBATCH --ntasks=48
#SBATCH --cpus-per-task=1
#SBATCH --time=1:20:00
#SBATCH --exclusive
#SBATCH --qos=debug
mpirun -np ${SLURM_NTASKS} ./Alya.x CASE_NAME
\end{lstlisting}

