\section{Experimental set-up}

This section explains the experimental set-up for the study. This includes the compilation of the application and its compilation parameters, the first runs of the application, the first traces of the application and finally the automation scripts for running and gathering data from multiple runs. 

\subsection{Compilation}

Alya is build using GNU/Make\cite{gnumake}. The Makefile necessary to compile the application is generated by a configure script written in Perl that parses the options and flags in the \textit{config.in} file. Table \ref{flagsintel} shows a summary of options toggled in the configuration. Notice that: we set the compiler to \textit{mpif90}, that is, the MPI compiler that wraps the Intel compiler,  we link to the Intel Math Kernel libraries\cite{intelmkl}, we explicitly tell the compiler that we want to use the Intel AVX-512 vectorial unit, to perform optimizations based on the architecture and to try to perform optimizations among all the files of the code.  In summary, we tried to achieve maximum performance from just tweaking compilation parameters.

\begin{table}[htbp]
\centering
%\refstepcounter{table}
\begin{tabular}{l|l|l} 
\toprule
\textbf{Flag}     & \textbf{Description}         & \textbf{Value}                                                                       \\ 
\hline
F77      & Fortran 77 compiler & mpif90                                                                      \\ 
\hline
F90      & Fortran 90 compiler & mpif90                                                                      \\ 
\hline
EXTRALIB & Libraries linked    & -lmkl\_core -lmkl\_sequential                                               \\ 
\hline 
FOPT     & Optimization level  & -O3                                                                         \\ 
\hline
OPTFLAGS & Optimization flags  & -xCORE-AVX512 -mtune=skylake -ipo                                           \\
\bottomrule
\end{tabular}
\caption{Configuration flags for Intel MPI Alya compilation.}
\label{flagsintel}
\end{table}

Once we have the \textit{config.in} file ready, we need to proceed to the actual compilation. Listing \ref{buildintel} shows the simplified build script. First, we load the Intel modules which enable the compilation using the Intel toolchain required. Then, we run the configure script, that will parse the \textit{config.in the} file we previously set and generate the Makefile. Finally using the generated Makefile we:

\begin{itemize}
  \item Compile the \textit{Metis4} partitioner.
  \item Compile the Cantera wrapper for Alya.
  \item Compile Alya telling the Makefile we enabled Cantera.
\end{itemize}

\begin{lstlisting}[language=sh, caption={Building Alya}, label={buildintel}]
module load intel mkl impi
./configure # -x nastin chemic parall temper
make metis4
make cantera4alya 
make CANTERA=1
\end{lstlisting}

With just that we got our Alya binary!

\subsection{Running set-up}

For running an HPC application, we need the binary and the input set. We know how to get the binary, and we are given the input set, meaning, we are ready for running the application.

An Alya input set is a folder with different files. Each file describes the mechanisms and parameters each module uses and defines the domain. All the input set files begin with the same name, and this is the case name. Listing \ref{runlinealya} shows the command to launch an Alya run assuming the binary is copied to the input set folder, and we are located inside that folder.

\begin{lstlisting}[language=sh, caption={Running Alya with 48 processes.}, label={runlinealya}]
mpirun -np 48 ./Alya.x CASE_NAME
\end{lstlisting}


Since we are running in a production HPC environment, we have to use the resource manager to run isolated and with the requested resources. Listing \ref{jobalya} shows how to run Alya using a Slurm script, requesting one node. Remember each MareNostrum4 node has 48 CPUs, and we are running with pure MPI. Therefore we set the \textit{cpus-per-task} flag to one and the \textit{ntasks} to 48. Other flags present are the output flag that tells the job manager the file to write the script's output, the time flag which we must provide to make sure that the job gets killed if it gets stalled and the exclusive flag tells Slurm that we do not want to share the resources.

\begin{lstlisting}[language=sh, caption={Running Alya with SLURM.}, label={jobalya}]
#!/bin/bash
#SBATCH --job-name=alya-test
#SBATCH --output=alya.out
#SBATCH --ntasks=48
#SBATCH --cpus-per-task=1
#SBATCH --time=1:20:00
#SBATCH --exclusive
mpirun -np ${SLURM_NTASKS} ./Alya.x CASE_NAME
\end{lstlisting}

\subsection{Tracing set-up}

In Section \ref{extrae} we presented a basic example of running extrae. Listing \ref{alyaextrae} shows the integration of Listing \ref{jobalya} with extrae. Notice that the structure is the same than the extrae example substituting \texttt{"BINARY"} with the Alya execution line (Alya.x f1d) 

\begin{lstlisting}[language=sh, caption={Running Alya with extrae and SLURM}, label={alyaextrae}]
#!/bin/bash
#SBATCH --job-name=alya-trace
#SBATCH --output=alya.out
#SBATCH --ntasks=48
#SBATCH --cpus-per-task=1
#SBATCH --time=1:20:00
#SBATCH --exclusive

export EXTRAE_HOME=#path to extrae installation
export EXTRAE_CONFIG_FILE=#path to extrae.xml

EXEC="env LD_PRELOAD=${EXTRAE_HOME}/lib/libmpitracef.so Alya.x f1d"
mpirun -np ${SLURM_NTASKS} ${EXEC}
\end{lstlisting}

\subsection{Wrap-up and automation}

This Section presents the scripts developed in order to gather all the data. 

First, we developed a parametrized script that enables us to run with different options without manually modifying the job script. In this case, we decided to leave placeholders in the job script and then set them with \texttt{GNU/sed}\footnote{\url{https://www.gnu.org/software/sed/}}. Listing \ref{alyageneral} shows the implementation, the convention for the placeholders is \texttt{\%\%PARAMETER\_NAME\%\%}. We set the trace location outside the folder where we execute, so all traces are stored in an independent folder. 

\begin{lstlisting}[language=sh, caption={Parametrized alya script.}, label={alyageneral}]
#!/bin/bash
#SBATCH --job-name=alya-%%MPI%%-%%TRACE%%
#SBATCH --output=alya.out
#SBATCH --ntasks=%%MPI%%
#SBATCH --cpus-per-task=1
#SBATCH --time=1:20:00
#SBATCH --exclusive

TRACE=%%TRACE%%

if [[ ${TRACE}==1 ]]; then 
    export EXTRAE_HOME=#path to extrae installation
    export EXTRAE_CONFIG_FILE=#path to extrae.xml
    TRACENAME="../traces/Alya-${SLURM_NTASKS}.prv"
    EXEC="env LD_PRELOAD=${EXTRAE_HOME}/lib/libmpitracef.so Alya.x f1d"
else
    EXEC="./Alya.x f1d"
fi

mpirun -np ${SLURM_NTASKS} ${EXEC}

if [[ ${TRACE}==1 ]]; then
    $EXTRAE_HOME/bin/mpi2prv -f TRACE.mpits -o ${TRACENAME} -no-keep-mpits
fi
\end{lstlisting}

Once we have the parametrized script, we developed a \textit{launcher} script in charge of submitting to SLURM de jobs requested. Listing \ref{alyalaunch} shows the implementation. For each job requested, we copy the input set so the jobs can be executed in parallel. We copy the parametrized job script into the copied input set, set the parameters using sed, and finally submit the job using SLURM.

\begin{lstlisting}[language=sh, caption={Alya launcher script.}, label={alyalaunch}]
function submit_job {
  NAME=alya-${MPI}-${TRACE} 
  cp -r base ${NAME}

  sed -i "s/%%TRACE%%/${TRACE}/g" ${NAME}/job.sh
  sed -i "s/%%MPI%%/${MPI}/g" ${NAME}/job.sh

  cd ${NAME}
  sbatch job.sh
  cd -
}

\end{lstlisting}

Listing \ref{alyalaunchexample} shows the usage of the \texttt{submit\_job} function with the purpose of executing Alya for 1, 2, 4, 8 and 16 nodes with tracing enabled and not.

\begin{lstlisting}[language=sh, caption={Alya job launch example.}, label={alyalaunchexample}]
for MPI in 48 96 192 384 768; do
  TRACE=1
  submit_job
  TRACE=0
  submit_job
done
\end{lstlisting}
